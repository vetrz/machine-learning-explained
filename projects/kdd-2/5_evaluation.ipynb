{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4535e7b0",
   "metadata": {},
   "source": [
    "# 1. Seleção"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4dc53",
   "metadata": {},
   "source": [
    "Nesta primeira etapa, definimos a base do nosso projeto.\n",
    "\n",
    "+ O que fizemos: Extraímos o Adult Data Set (Censo) do repositório da UCI.\n",
    "\n",
    "+ A nossa escolha: Separamos os dados estruturais (as features, como idade, educação e horas de trabalho) da nossa variável-alvo (target), que é prever se a renda da pessoa ultrapassa os 50 mil dólares anuais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca13efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff60c441",
   "metadata": {},
   "outputs": [],
   "source": [
    "adult = fetch_ucirepo(id=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883d98e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = adult.data.features\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a934b777",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = adult.data.targets\n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e33238",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"income\"] = target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff35ccc",
   "metadata": {},
   "source": [
    "# 2. Pré-Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a25400b",
   "metadata": {},
   "source": [
    "Aqui, investigamos a qualidade dos dados e lidamos com valores ausentes (como os campos em branco ou preenchidos com \"?\").\n",
    "\n",
    "+ O que fizemos: Comparamos empiricamente várias estratégias de limpeza para ver qual ajudava mais o modelo.\n",
    "\n",
    "+ A nossa escolha: Descobrimos que não remover linhas com dados faltantes era a melhor opção. Ao apagar dados com \"?\" ou nulos, nossa acurácia caiu para 86,56%. A escolha final foi tratar esses dados ausentes preenchendo-os com a moda (valor mais frequente) e removendo as colunas \"education-num\", \"fnlwgt devido, a feature \"education-num\" ser apenas um id de \"education\" feature, falando sobre \"fnlwgt\" ajuste na distribuição seguindo uma tranformação logmod."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6960235",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>Metodos</th>\n",
    "      <th>acurácia</th>\n",
    "      <th>Modelo</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td> Sem alterações no preprocessamento </td>\n",
    "      <td> 87,69% </td>\n",
    "      <td rowspan=\"9\" style=\"vertical-align: middle;\">XGBoost</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Removendo linhas com Valores nulos</td>\n",
    "      <td> 87,49% </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Substituindo Valores nulos pela moda</td>\n",
    "      <td> 87,78% </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Substituir nulos pela moda e dropar duplicados</td>\n",
    "      <td> 87,54% </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Remover linhas com nulos e duplicados</td>\n",
    "      <td>87,19%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Remover linhas com valores \"?\"</td>\n",
    "      <td>86,56%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Substituir valores \"?\" pela moda</td>\n",
    "      <td>87,59%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Substituir pela moda valores nulos e \"?\"</td>\n",
    "      <td>87,63%</td>\n",
    "    </tr>\n",
    "    <tr style=\"font-weight: bold;\">\n",
    "      <td>Valores nulos pela moda e drop de 2 colunas e ajuste na distribuição de fnlwgt</td>\n",
    "      <td>87,94%</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefe8047",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape inicial: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8757292",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cb4f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "corr = df.select_dtypes(include=['number']).corr().abs()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(corr, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f768f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_cols = df.select_dtypes(include=['int', 'float'])\n",
    "\n",
    "for col in num_cols:\n",
    "    plt.figure(figsize=(10,4))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f'Distribution: {col}')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.boxenplot(df[col])\n",
    "    plt.title(f'Boxplot: {col}')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a2a443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df['fnlwgt_logmod'] = np.sign(df['fnlwgt']) * np.log1p(np.abs(df['fnlwgt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e529480",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valores ausentes por coluna:\")\n",
    "print(df.isna().sum())\n",
    "print(f\"\\nTotal de linhas com valores ausentes: {df.isna().any(axis=1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ec9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nan_columns = df.columns[df.isna().any()].tolist()\n",
    "print(f\"Colunas com valores ausentes: {nan_columns}\")\n",
    "for column in nan_columns:\n",
    "    print(f\"Preenchendo valores ausentes na coluna '{column}' com a moda.\")\n",
    "    df[column] = df[column].fillna(df[column].mode()[0])\n",
    "print(\"\\nVerificação:\")\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    n_unique = df[col].nunique()\n",
    "    print(f\"{col} ({n_unique} valores únicos):\")\n",
    "    print(df[col].unique())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95abd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Valores originais de 'income':\")\n",
    "print(df[\"income\"].unique())\n",
    "\n",
    "df[\"income\"] = df[\"income\"].str.replace(\".\", \"\").str.strip()\n",
    "\n",
    "print(\"\\nValores após limpeza:\")\n",
    "print(df[\"income\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b230b090",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribuição de 'occupation':\")\n",
    "print(df[\"occupation\"].value_counts())\n",
    "print(f\"\\nRegistros com '?': {(df['occupation'] == '?').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261bbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribuição de 'native-country':\")\n",
    "print(df[\"native-country\"].value_counts())\n",
    "print(f\"\\nRegistros com '?': {(df['native-country'] == '?').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4593f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Colunas antes da remoção:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "df = df.drop(columns=[\"education-num\", \"fnlwgt\"])\n",
    "\n",
    "print(\"\\nColunas após remoção:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"\\nShape final: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd90085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_cols = ['fnlwgt_logmod']\n",
    "\n",
    "for col in num_cols:\n",
    "    plt.figure(figsize=(10,4))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f'Distribution: {col}')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    sns.boxenplot(df[col])\n",
    "    plt.title(f'Boxplot: {col}')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b5ade",
   "metadata": {},
   "source": [
    "# 3. Transformação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fad8d7",
   "metadata": {},
   "source": [
    "Modelos matemáticos não leem textos e são sensíveis a escalas numéricas diferentes. Nesta fase, traduzimos os dados para o modelo.\n",
    "\n",
    "+ O que fizemos: Transformamos variáveis categóricas (como \"Private\" e \"Local-gov\") em números através do One-Hot Encoding e ajustamos escalas numéricas.\n",
    "\n",
    "+ A nossa escolha: Testamos tanto a Padronização quanto a Normalização dos dados. Como ambas empataram em 87,94% de acurácia, comprovamos que o modelo escolhido (baseado em árvores) é robusto e invariante à escala."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349e4eef",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>Metodos</th>\n",
    "      <th>acurácia</th>\n",
    "      <th>Modelo</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "  <tr>\n",
    "      <td><b>Sem métodos de scaler</b></td>\n",
    "      <td><b>87,94%</b></td>\n",
    "      <td rowspan=\"3\" style=\"vertical-align: middle;\">XGBoost</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>Padronizando os dados</b></td>\n",
    "      <td><b>87,94%</b></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td><b>Normalizando os dados</b></td>\n",
    "      <td><b>87,94%</b></td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21d0d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e64cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape do dataset: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df02bf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = (df[\"income\"] == \">50K\").astype(int)\n",
    "X = df.drop(columns=[\"income\"])\n",
    "\n",
    "print(f\"Shape de X: {X.shape}\")\n",
    "print(f\"Shape de y: {y.shape}\")\n",
    "print(\"\\nDistribuição da variável target:\")\n",
    "print(y.value_counts())\n",
    "print(\"\\nProporção:\")\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7cc7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(f\"Variáveis numéricas ({len(numerical_cols)}):\")\n",
    "print(numerical_cols)\n",
    "print(f\"\\nVariáveis categóricas ({len(categorical_cols)}):\")\n",
    "print(categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y,\n",
    ")\n",
    "\n",
    "print(f\"Shape de X_train: {X_train.shape}\")\n",
    "print(f\"Shape de X_test: {X_test.shape}\")\n",
    "print(f\"Shape de y_train: {y_train.shape}\")\n",
    "print(f\"Shape de y_test: {y_test.shape}\")\n",
    "\n",
    "print(\"\\nDistribuição em treino:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(\"\\nDistribuição em teste:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac3103",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aplicando One-Hot Encoding...\")\n",
    "\n",
    "X_train_encoded = pd.get_dummies(X_train, columns=categorical_cols, drop_first=False)\n",
    "X_test_encoded = pd.get_dummies(X_test, columns=categorical_cols, drop_first=False)\n",
    "\n",
    "print(\"\\nShape após encoding:\")\n",
    "print(f\"X_train_encoded: {X_train_encoded.shape}\")\n",
    "print(f\"X_test_encoded: {X_test_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9edb773",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Alinhando colunas entre treino e teste...\")\n",
    "\n",
    "X_train_encoded, X_test_encoded = X_train_encoded.align(\n",
    "    X_test_encoded,\n",
    "    join=\"left\",\n",
    "    axis=1,\n",
    "    fill_value=0,\n",
    ")\n",
    "\n",
    "print(\"\\nShape após alinhamento:\")\n",
    "print(f\"X_train_encoded: {X_train_encoded.shape}\")\n",
    "print(f\"X_test_encoded: {X_test_encoded.shape}\")\n",
    "print(f\"\\nColunas idênticas: {X_train_encoded.columns.equals(X_test_encoded.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2885d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Colunas numéricas a serem normalizadas:\")\n",
    "print(numerical_cols)\n",
    "\n",
    "existing_num_cols = [col for col in numerical_cols if col in X_train_encoded.columns]\n",
    "print(\"\\nColunas numéricas presentes:\")\n",
    "print(existing_num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db207eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = False\n",
    "scaler = StandardScaler() if standard else MinMaxScaler()\n",
    "\n",
    "X_train_encoded[existing_num_cols] = scaler.fit_transform(X_train_encoded[existing_num_cols])\n",
    "X_test_encoded[existing_num_cols] = scaler.transform(X_test_encoded[existing_num_cols])\n",
    "\n",
    "print(\"\\nEstatísticas das variáveis numéricas no treino:\")\n",
    "print(X_train_encoded[existing_num_cols].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88fbc04",
   "metadata": {},
   "source": [
    "# 4. Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f232a5ab",
   "metadata": {},
   "source": [
    "Esta é a fase de aprendizado, onde aplicamos o algoritmo principal para extrair os padrões preditivos da base.\n",
    "\n",
    "+ O que fizemos: Definimos o XGBoost como nosso modelo principal devido à sua alta performance com dados tabulares.\n",
    "\n",
    "+ A nossa escolha: Para tentar melhorar o modelo, aplicamos técnicas de busca de hiperparâmetros (como o Grid Search e Random Search). Curiosamente, observamos que os parâmetros padrão do XGBoost já eram extremamente otimizados para essa base (87,94%), tendo um desempenho até levemente superior à busca exaustiva que fizemos (87,88%), o que nos ensinou sobre o risco de overfitting na otimização."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0f9cae",
   "metadata": {},
   "source": [
    "<div>\n",
    "<style scoped>\n",
    "    .dataframe tbody tr th:only-of-type {\n",
    "        vertical-align: middle;\n",
    "    }\n",
    "\n",
    "    .dataframe tbody tr th {\n",
    "        vertical-align: top;\n",
    "    }\n",
    "\n",
    "    .dataframe thead th {\n",
    "        text-align: right;\n",
    "    }\n",
    "</style>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th>Metodos</th>\n",
    "      <th>acurácia</th>\n",
    "      <th>Modelo</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "  <tr>\n",
    "      <td><b>Sem métodos de hiperparametros</b></td>\n",
    "      <td><b>87,94%</b></td>\n",
    "      <td rowspan=\"4\" style=\"vertical-align: middle;\">XGBoost</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Grid Search</td>\n",
    "      <td>87,88%</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Random Search</td>\n",
    "      <td>87,88%</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5619c637",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb9196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"model\": LogisticRegression(max_iter=1000, random_state=42)\n",
    "    },\n",
    "\n",
    "    \"Random Forest\": {\n",
    "        \"model\": RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    },\n",
    "\n",
    "    \"XGBoost\": {\n",
    "        \"model\": XGBClassifier(random_state=42, eval_metric=\"logloss\", n_jobs=-1),\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b8c044",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for name, mp in models.items():\n",
    "    model = mp['model']\n",
    "    model.fit(X_train_encoded, y_train.values.ravel())\n",
    "    test_score = model.score(X_test_encoded, y_test)\n",
    "\n",
    "    results[name] = {\n",
    "        \"model\": model,\n",
    "        \"params\": model.get_params(),\n",
    "        \"test_score\": test_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d0f6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    \"model\": list(results.keys()),\n",
    "    \"test_score\": [results[m][\"test_score\"] for m in results],\n",
    "}).sort_values(\"test_score\", ascending=False)\n",
    "\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = comparison.iloc[0][\"model\"]\n",
    "best_model = results[best_model_name]['model']\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Test Accuracy: {results[best_model_name]['test_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69b4322",
   "metadata": {},
   "source": [
    "# 5. Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06667995",
   "metadata": {},
   "source": [
    "A fase final serviu para comprovar matematicamente que nosso modelo funciona e pode generalizar o conhecimento.\n",
    "\n",
    "+ O que fizemos e concluímos: Avaliamos todas as modificações anteriores mantendo a métrica de Acurácia como nossa bússola. Conseguimos provar que, com o tratamento correto de dados categóricos e respeitando a natureza do algoritmo (sem deletar dados cegamente), estabilizamos a capacidade preditiva do modelo em quase 88% de acerto no mundo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b22e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test_encoded)\n",
    "print(classification_report(y_test, y_pred, target_names=[\"<=50K\", \">50K\"]))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e8dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"<=50K\", \">50K\"],\n",
    "    yticklabels=[\"<=50K\", \">50K\"],\n",
    ")\n",
    "plt.ylabel(\"Valor Real\")\n",
    "plt.xlabel(\"Valor Predito\")\n",
    "plt.title(\"Matriz de Confusão\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Verdadeiros Negativos (<=50K corretos): {cm[0,0]}\")\n",
    "print(f\"Falsos Positivos (previu >50K, era <=50K): {cm[0,1]}\")\n",
    "print(f\"Falsos Negativos (previu <=50K, era >50K): {cm[1,0]}\")\n",
    "print(f\"Verdadeiros Positivos (>50K corretos): {cm[1,1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-explained (3.13.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
